{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start Jupyter in the environment 'Python 3.10.0 (~/Documents/python/open-class-room/!!! projets/projet 02 - Participez à un concours sur la Smart /.venv/bin/python)'. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import display\n",
    "from functions import *\n",
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 161)\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/fr.openfoodfacts.org.products.csv\", sep='\\t', decimal='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uniformisation du type des colones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, col in data.iloc[:, [0,3,5,19,20,24,25,26,27,28,35,36,37,38,39,48]].iteritems():\n",
    "    if len((col.dropna().apply(type).unique())) != 1:\n",
    "        print(name)\n",
    "del(name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['code', 'created_t', 'last_modified_t']].applymap(type).apply(\n",
    "    lambda col: col.dropna().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hormis trois colones dont le code barre et les métadonnées, la seul raisons de l'hétérogénéité des colones est lié la présence de valeurs NaN identifiés comme réel. On choisit de convertir chaque colone vers son type principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densité du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le dataset est constitué d'un ensemble de plus de 300 000 individus constitué de 162 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on commence par évaluer la densité du dataset pour pour mieux évaluer la quantité l'information présente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densityByCountry (country=None):\n",
    "    byCountry = data[data.countries.str.contains(country)] if country else data\n",
    "    (100-byCountry.isna().sum().sort_values()/len(byCountry)*100).plot()\n",
    "    plt.gca().set_xticks(range(0, byCountry.shape[1]+1, 20), range(0, byCountry.shape[1]+1, 20))\n",
    "    plt.show()\n",
    "\n",
    "densityByCountry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la moitié des colones sont vides ou presque, un peu plus d'une trentaines sont remplis à 60% ou plus. on choisit d'examiner ces colones de plus prés pour mieux comprendre la nature de l'information disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pd.cut(100-data.isna().sum().sort_values()/len(data)*100, range(60, 101, 10)).dropna()))\n",
    "pd.cut(100-data.isna().sum().sort_values()/len(data)*100, range(60, 101, 10)).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les colones les plus remplies sont constitués de méta données :\n",
    "- created_datetime, created_t : date de création de page sur open fact food\n",
    "- last_modified_datetime, last_modified_t : date de dernière modification de page sur open fact food\n",
    "- creator : contributeur de l'ajout initial à la base open fact food\n",
    "- states, states_tags, states_fr : état de complétion de la saisie\n",
    "- url : adresse web de la page du produit sur le site open fact food\n",
    "\n",
    "=> ces données apportent peu d'information sur le produit en lui même et sont écartés pour l'instant\n",
    "\n",
    "des données descriptive sur le produit :\n",
    "- code : code barre EAN 13 du produit\n",
    "- countries, countries_tags, countries_fr : pays ou à eu lieu la saisie, et donc la vente\n",
    "- product_name, brands, brands_tags : nom du produit et marque\n",
    "\n",
    "=> éviter la redondance et choisir une représentation unifié pour les pays et marques. le code barre est sortie de l'étude pour l'instant, au besoin vérifier la conformité et unicité\n",
    "\n",
    "des données sur la qualité du le produit :\n",
    "- ingredients_text : liste des ingrédient\n",
    "- additives, additives_n : liste et nombres des additifs\n",
    "- ingredients_from_palm_oil_n, ingredients_that_may_be_from_palm_oil_n: source d'huile de palme\n",
    "- nutrition-score-uk_z, nutrition-score-fr_100g, nutrition_grade_fr : valeur nutritive\n",
    "\n",
    "=> le nutriscore est la donnée la plus synthétique et porteuse d'information, il faudra vérifier si la redondance france / angleterre peux apporter de l'information. les autres informations sont écartés pour l'instant\n",
    "\n",
    "des données quantitatives sur le produit :\n",
    "- energy_100g, sugars_100g, saturated-fat_100g, sodium_100g : composante negative du nutriscore\n",
    "- fiber_100g, proteins_100g : composante positive du du nutriscore\n",
    "- salt_100g, fat_100g, carbohydrates_100g : données complémentaires possiblement corrélées \n",
    "\n",
    "=> ces données pourraient servir à inférer le nutriscore et seront conservées\n",
    "\n",
    "note pour plus tard : serving_size\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = data.dropna(axis=1, thresh=len(data)*0.6).copy()\n",
    "food.drop(['code', 'created_datetime', 'created_t' , 'last_modified_datetime', 'last_modified_t', 'creator', 'states', 'states_tags', 'states_fr', 'url'], axis=1, inplace=True)\n",
    "food.drop(['ingredients_text', 'additives', 'additives_n', 'ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n'], axis=1, inplace=True)\n",
    "food.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix de représentation unifié pour les pays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( data[['countries', 'countries_tags', 'countries_fr']].isnull().sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y'a autant d'information manquante dans chaque colones représentant les pays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['countries', 'countries_tags', 'countries_fr'] :\n",
    "    print(f\"{col} : {food[col].str.split(',').dropna().explode().nunique()}\")\n",
    "del(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pour une même quantité d'information, la colone countries_fr demandera le moins d'effort f'uniformisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.drop(['countries', 'countries_tags'], axis=1, inplace=True)\n",
    "food.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix de représentation unifié pour les marques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( data[['brands', 'brands_tags']].isnull().sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y'a une même quantitée d'information manquante dans chaque colones représentant les marques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(food['brands'].str.split(',').dropna().explode().nunique())\n",
    "display(food['brands_tags'].str.split(',').dropna().explode().nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "à quantité d'information identique, la colone brands_tags demandera le moins d'effort f'uniformisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.drop(['brands_tags'], axis=1, inplace=True)\n",
    "food.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix d'une représentation unifié pour le nutriscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "existe une part d'information dans l'une des données du nutriscore présente dans l'une où l'autre ne l'est pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(food['nutrition-score-uk_100g'].isna()^food['nutrition-score-fr_100g'].isna()).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "existe t'il un écart entre ces deux valeurs ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(food['nutrition-score-uk_100g'] - food['nutrition-score-fr_100g']).abs(\n",
    "    ).describe(percentiles=[0.68, 0.96, 0.97])[['mean', 'std', '96%', '97%']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il existe une différence pour un peu moins de 4% des nutriscore renseignés. un écart trop important peux être indicatif d'une erreur. elles sont gardés toutes les deux pour aider aux nettoyages des variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix d'une représentation unifié pour le sel et sodium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'équivalent sel pour 100g est de 2,54 fois la quantitée de sodium pour 100g, ce qui se vérifie dans les données avec une erreur relative supérieur à 3% pour moins de 0.1% des produits présents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saltSodium = food.loc[ (food.salt_100g>0) & (food.sodium_100g>0) ,['salt_100g', 'sodium_100g']]\n",
    "display(((saltSodium.salt_100g - 2.54*saltSodium.sodium_100g) / saltSodium.salt_100g * 100).abs().describe(percentiles=[0.99, 0.999, 1])[['99%', '99.9%', 'max']])\n",
    "del(saltSodium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il existe un peu complémentarité dans les données qui pourront servir à remplir les valeurs manquantes après nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(food.salt_100g.isna()^food.sodium_100g.isna()).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage des variables choisies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des nutriscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nutriscore est issue du 'Nutrient Profiling Model' mis au point par la 'Food Standards Agency' (FSA) en 2004. il va de -15 (favorable) à 40 (défavorable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food[['nutrition-score-uk_100g', 'nutrition-score-fr_100g']].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les deux colones sont bien interprétés comme des entiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food[['nutrition-score-uk_100g', 'nutrition-score-fr_100g']].describe().loc[['min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'ensemble des colones se référant au nutriscore sont conformes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du nutriscore est dérivée le nutrigrade qui à pour fonction de rendre le score plus accessible au grand public. Il va de 'A' (favorable) à 'E' défavorable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(food['nutrition_grade_fr'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la colone nutrigrade est conforme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'écart de valeur entre nutrition-score-uk_100g et nutrition-score-fr_100g peux être indicatif d'une erreur dans le calcul des scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(food['nutrition-score-uk_100g'] - food['nutrition-score-fr_100g']).abs().value_counts().sort_index()[1:].plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les erreurs inférieures à 4 sont à la fois peux significative et peu nombreuses et sont gardés. Les autres sont marqués NA pour traitement ultérieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[(food['nutrition-score-uk_100g'] - food['nutrition-score-fr_100g']).abs()>=5,\n",
    "    ['nutrition-score-uk_100g', 'nutrition-score-fr_100g']] = [pd.NA, pd.NA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les écarts marqués comme erreurs, les colones nutrition-score-uk_100g et nutrition-score-fr_100g deviennent redondantes. On ne garde que nutrition-score-fr_100g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.drop(['nutrition-score-uk_100g'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des valeurs nutritionnels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nettoyages par analyse univariée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cas général"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les valeurs nutritive pour 100g (hors énergie) sont des nombres réels compris entre 0 et 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(food.loc[:,'energy_100g':'salt_100g'].dtypes == 'Float64').all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = food.loc[:,'fat_100g':'salt_100g'].applymap(\n",
    "    lambda x: (x<0) or (x>100), na_action='ignore').apply(pd.Series.value_counts)\n",
    "print(f'{temp.loc[True,:].sum()} valeurs non conformes')\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soit 246 valeurs non conformes que l'on remplace par NA pour autre traitement ultérieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in food.loc[:,'fat_100g':'salt_100g']:\n",
    "    food.loc[(food[col]<0) | (food[col]>100), col] = pd.NA\n",
    "food.loc[:,'fat_100g':'sodium_100g'].copy().describe().loc[['min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cas spécifique du Sodium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concernant le sodium en relation avec le sel, à 100g de sel correspond 39.4g de sodium, il ne peut y avoir donc plus de 39.4g de sodium par 100g d'aliment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = food.sodium_100g.dropna().apply(lambda x: (x<0) or (x>39.4)).value_counts()\n",
    "print(f\"{temp[True]} valeurs non conformes\")\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soit 156 valeurs non conformes que l'on remplace par NA pour autre traitement ultérieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[(food.sodium_100g<0) | (food.sodium_100g>39.4), 'sodium_100g'] = pd.NA\n",
    "food.sodium_100g.describe().loc[['min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas spécifique de l'énergie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la quantité d'énergie max par gramme étant de 38kJ, l'énergie pour 100g doit être compris entre 0 et 3800Kj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = food.energy_100g.dropna().apply(lambda x: (x<0) or (x>3800)).value_counts()\n",
    "print(f\"{temp[True]} valeurs non conformes\")\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soit 357 valeurs non conformes que l'on remplace par NA pour autre traitement ultérieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[(food.energy_100g<0) | (food.energy_100g>3800), 'energy_100g'] = pd.NA\n",
    "food.energy_100g.describe().loc[['min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nettoyages par analyse multivariée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zScoreAbs(col):\n",
    "    return ((food[col]-food[col].mean())/food[col].std()).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zMaxIs(col, cols):\n",
    "    temp = pd.Series(True, index=food[col].index)\n",
    "    for coln in cols :\n",
    "        temp = temp & (food[col] > food[coln])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['carbohydrates_z', 'sugars_z', 'fiber_z']:\n",
    "    food[col] = zScoreAbs(col.replace('z','100g'))\n",
    "\n",
    "err3 = food.carbohydrates_100g < food.sugars_100g + food.fiber_100g\n",
    "food.loc[err3 & zMaxIs('carbohydrates_z',['sugars_z', 'fiber_z']), 'carbohydrates_100g'] = pd.NA\n",
    "food.loc[err3 & zMaxIs('sugars_z',['carbohydrates_z', 'fiber_z']), 'sugars_100g'] = pd.NA\n",
    "food.loc[err3 & zMaxIs('fiber_z',['carbohydrates_z', 'sugars_z']), 'fiber_100g'] = pd.NA\n",
    "\n",
    "err2 = food.carbohydrates_100g < food.fiber_100g\n",
    "food.loc[err2 & zMaxIs('fiber_z',['carbohydrates_z']), 'fiber_100g'] = pd.NA\n",
    "food.loc[err2 & zMaxIs('carbohydrates_z',['fiber_z']), 'carbohydrates_100g'] = pd.NA\n",
    "\n",
    "err1 = food.carbohydrates_100g < food.sugars_100g\n",
    "food.loc[err1 & zMaxIs('sugars_z',['carbohydrates_z']), 'sugars_100g'] = pd.NA\n",
    "food.loc[err1 & zMaxIs('carbohydrates_z',['sugars_z']), 'carbohydrates_100g'] = pd.NA\n",
    "\n",
    "print(err1.sum()+err2.sum()+err3.sum())\n",
    "food = food.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['fat_z', 'saturated-fat_z']:\n",
    "    food[col] = zScoreAbs(col.replace('z','100g'))\n",
    "\n",
    "err = food.fat_100g < food['saturated-fat_100g']\n",
    "food.loc[err & zMaxIs('saturated-fat_z',['fat_z']), 'saturated-fat_100g'] = pd.NA\n",
    "food.loc[err & zMaxIs('fat_z',['saturated-fat_z']), 'fat_100g'] = pd.NA\n",
    "\n",
    "print(err.sum())\n",
    "food = food.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food['proteins_z'] = zScoreAbs('proteins_100g')\n",
    "\n",
    "err4 = food[['fat_100g', 'carbohydrates_100g', 'proteins_100g']].sum(axis=1)>100\n",
    "food.loc[err4 & zMaxIs('fat_z',['carbohydrates_z', 'proteins_z']), 'fat_100g'] = pd.NA\n",
    "food.loc[err4 & zMaxIs('carbohydrates_z',['fat_z', 'proteins_z']), 'carbohydrates_100g'] = pd.NA\n",
    "food.loc[err4 & zMaxIs('proteins_z',['fat_z', 'carbohydrates_z']), 'proteins_100g'] = pd.NA\n",
    "\n",
    "err3 = food[['carbohydrates_100g', 'proteins_100g']].sum(axis=1)>100\n",
    "food.loc[err3 & zMaxIs('carbohydrates_z',['proteins_z']), 'carbohydrates_100g'] = pd.NA\n",
    "food.loc[err3 & zMaxIs('proteins_z',['carbohydrates_z']), 'proteins_100g'] = pd.NA\n",
    "\n",
    "err2 = food[['fat_100g', 'proteins_100g']].sum(axis=1)>100\n",
    "food.loc[err2 & zMaxIs('fat_z',['proteins_z']), 'fat_100g'] = pd.NA\n",
    "food.loc[err2 & zMaxIs('proteins_z',['fat_z']), 'proteins_100g'] = pd.NA\n",
    "\n",
    "err1 = food[['fat_100g', 'carbohydrates_100g']].sum(axis=1)>100\n",
    "food.loc[err1 & zMaxIs('fat_z',['carbohydrates_z']), 'fat_100g'] = pd.NA\n",
    "food.loc[err1 & zMaxIs('carbohydrates_z',['fat_z']), 'carbohydrates_100g'] = pd.NA\n",
    "\n",
    "print(err1.sum()+err2.sum()+err3.sum()+err4.sum())\n",
    "food = food.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food['energy_z'] = zScoreAbs('energy_100g')\n",
    "\n",
    "err = ( (food['energy_100g'] - 17*food['proteins_100g'] - 37*food['fat_100g'] - 17*food['carbohydrates_100g'] - 8*food['fiber_100g']) / food['energy_100g'] ).abs() > 0.1\n",
    "food.loc[err & zMaxIs('energy_z', ['proteins_z', 'fat_z', 'carbohydrates_z', 'fiber_z']), 'energy_100g'] = pd.NA\n",
    "food.loc[err & zMaxIs('proteins_z', ['energy_z', 'fat_z', 'carbohydrates_z', 'fiber_z']), 'proteins_100g'] = pd.NA\n",
    "food.loc[err & zMaxIs('fat_z', ['energy_z', 'proteins_z', 'carbohydrates_z', 'fiber_z']), 'fat_100g'] = pd.NA\n",
    "food.loc[err & zMaxIs('carbohydrates_z', ['energy_z', 'proteins_z', 'fat_z', 'fiber_z']), 'carbohydrates_100g'] = pd.NA\n",
    "food.loc[err & zMaxIs('fiber_z', ['energy_z', 'proteins_z', 'fat_z', 'carbohydrates_z']), 'fiber_100g'] = pd.NA\n",
    "\n",
    "print(err.sum())\n",
    "food = food.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(col, err, err1, err2, err3, err4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### référence des liquides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les liquides au sens du nutriscore ont des valeurs nutritives faibles ou nulles qui pourraient les voir supprimé à tord, on cherche donc à les identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1er source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data[data.pnns_groups_2 == 'Alcoholic beverages'].index))\n",
    "food.drop(data[data.pnns_groups_2 == 'Alcoholic beverages'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nutri-Score modification applies to the following beverages : Mineral water, flavoured water,fruit juices, nectars, smoothies, vegetable juices, drinks with added sugar and/or sweeteners,teas, infusions or coffee reconstituted exclusively with water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boissons1 := data[\n",
    "   (data.pnns_groups_1.str.contains('Beverages', case=False) |\n",
    "    data.pnns_groups_2.str.contains('beverages|juices', case=False)) &\n",
    "   ~data.product_name.str.contains('plat|lait|milk|huile|oil|biscuit|cookie|gâteau|cake|lacté|soluble|soja|riz', case=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2éme source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boissons = 'boissons,|,boissons|boissons au|beverage|drink|water|juice|nectar|smoothie|lemonade|eau|jus|soda|limonades'\n",
    "boissons = 'beverage|drink|water|juice|nectar|smoothie|tea|infusion|coffee|lemonade|boisson|eau|jus|thé|café|soda|limonades'\n",
    "data[data.categories.str.contains(boissons, case=False) & \n",
    "    ~data.categories.str.contains('plat|lait|milk|huile|oil|biscuit|cookie|gâteau|cake|lacté', case=False) #s préparés\n",
    "].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soit 16021 boissons potentiels correspondant à la définition du nutriscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boissons = 'beverage|drink|water|juice|nectar|smoothie|tea|infusion|coffee|lemonade|boisson|eau|jus|thé|café|soda|limonades'\n",
    "erreurs = 'plat|lait|milk|huile|oil|biscuit|cookie|gâteau|cake|lacté|soja'\n",
    "len(boissons21 := data[data.categories.str.contains(boissons, case=False) &\n",
    "    ~data.serving_size.str.contains('g') &\n",
    "    ~data.categories.str.contains(erreurs, case=False) &\n",
    "    ~data.product_name.str.contains(erreurs, case=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dont 2957 boissons vendus au litre et considérées comme fiables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.categories.str.contains(boissons, case=False) & \n",
    "    (data.serving_size.isna() | data.serving_size.str.contains('g')) &\n",
    "    ~data.categories.str.contains('plat|lait|milk|huile|oil|biscuit|cookie|gâteau|cake|lacté', case=False) #s préparés\n",
    "].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et 13064 potentiel, parmi lesquels on écarte ceux vendus au grammes, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.categories.str.contains(boissons, case=False) & \n",
    "    data.serving_size.str.contains('g') &\n",
    "    ~data.categories.str.contains('plat|lait|milk|huile|oil|biscuit|cookie|gâteau|cake|lacté', case=False) #s préparés\n",
    "].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "réduisant les potentiels à 9221, parmi lesquels on écarte les erreurs évidentes sur les noms de produits et catégories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boissons = 'boissons,|,boissons|boissons au|beverage|drink|water|juice|nectar|smoothie|lemonade|eau|jus|soda|limonades'\n",
    "erreurs = '|'.join([',plant-based foods', 'biscotte', 'bloc de', 'capsule', 'cerneaux', 'chiches', 'compote', 'dosette', 'dénoyauté', 'déshydraté', 'effilées', 'en conserve', 'en poudre', 'farine', 'farines', 'filets de', 'flour', 'foie', 'fond de', 'fruits à coque', 'jambon', 'maceta', 'morceaux', 'moulu', 'moulus', 'open beauty', 'plant-based foods,', 'pois', 'poissons', 'ravioles', 'ravioli', 'rillette', 'rillettes', 'rondelles', 'râpée', 'sachet', 'saucisse', 'saucisses', 'soluble', 'sucre', 'surgelés', 'tortillas', 'veau', 'viande', 'épeautre'])\n",
    "len(boissons22 := data[\n",
    "     data.categories.str.contains(boissons, case=False) & \n",
    "     data.serving_size.isna() &\n",
    "    ~data.categories.str.contains('plat|lait|milk|huile|oil|biscuit|cookie|gâteau|cake|lacté', case=False) &\n",
    "    ~data.categories.str.contains(erreurs, case=False) &\n",
    "    ~data.product_name.str.contains(erreurs, case=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toutes sources confondus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soit 9538 boissons confirmées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boissonsIndex := boissons1.index.union(\n",
    "           boissons21.index).union(\n",
    "           boissons22.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qui peuvent être étendue à 11264 produits du même nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boissonsIndex := data[data.product_name.isin(data.loc[boissonsIndex, 'product_name'])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur un total de départ de 21534 potentiels dont la majorités sont soit des erreurs soit ne correspondent pas ne correspondent à la définition de boissons du nutriscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boissonsPotentielIndex := data[\n",
    "     data.pnns_groups_1.str.contains('beverages', case=False) |\n",
    "     data.pnns_groups_2.str.contains('beverages|juices', case=False) |\n",
    "     data.categories.str.contains('boissons|beverage|drink|water|juice|nectar|smoothie|tea|infusion|coffee|lemonade|eau|jus|thé|café|soda|limonades', case=False)\n",
    "].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pouvant être étendue à 47666 potentiels dont la majorités sont soit des erreurs soit ne correspondent pas ne correspondent à la définition de boissons du nutriscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boissonsPotentielIndex := data[data.product_name.isin(data.loc[boissonsPotentielIndex, 'product_name'])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on choisie de marquer comme non liquide les éléments non potentiels parmi ceux étant référencés par le pnns et les catégories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nourritureIndex := data[\n",
    "    data.pnns_groups_1.notna() |\n",
    "    data.pnns_groups_2.notna() |\n",
    "    data.categories.notna()\n",
    "].index.difference(boissonsPotentielIndex)) #TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et on étends au produit de même nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nourritureIndex := data[data.product_name.isin(data.loc[nourritureIndex, 'product_name'])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(boissons, boissons1, boissons21, boissons22, erreurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[food.index.intersection(boissonsIndex), 'isBeverage'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[food.index.intersection(nourritureIndex), 'isBeverage'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(food[food.isBeverage==1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(food[food.isBeverage==0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nettoyage des valeurs non inférables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les produits sans nutriscore nécessite la présence de valeurs nutritives pour les évaluer. La qualité de cette évaluation dépends de la quantitée d'information présente pour chaque produit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(food.loc[(food['nutrition_grade_fr'].isna() & food['nutrition-score-fr_100g'].isna()).index.difference(boissonsPotentielIndex), \n",
    "    'energy_100g':'sodium_100g'].isna().sum(axis=1).value_counts() / len(food) * 100).sort_index().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : make a graph of it ?\n",
    "((food.loc[(food['nutrition_grade_fr'].isna() & food['nutrition-score-fr_100g'].isna()).index.difference(boissonsPotentielIndex), \n",
    "    'energy_100g':'sodium_100g'].isna().sum(axis=1).value_counts() / len(food) * 100).sort_index()).loc[4:9].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit 0.2% à 12.3% de d'information perdus selon le niveau de qualité d'information choisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[4,'energy_100g':'sodium_100g']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les produits ayant 8 valeurs absentes n'offrent pas de base informative suffisante pour inférer le nutriscore et représentent un peu moins de 11% des produits. Leurs nombres et la faible quantitée informative qu'ils apporteraient contribueraient à faire baisser la qualité de l'informations\n",
    "\n",
    "les produits ayant 4, 5, 6 ou 7 valeurs absentes sont peu nombreux et la quantité d'information présente ne permettraient pas d'inférer une information de qualités. On choisit de ne pas les inclure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexDrop = food[(food['nutrition_grade_fr'].isna() & \n",
    "                  food['nutrition-score-fr_100g'].isna() & \n",
    "                 (food.loc[:,'energy_100g':'sodium_100g'].isna().sum(axis=1)>=4)\n",
    ")].index.difference(boissonsPotentielIndex)\n",
    "food.drop(indexDrop, inplace=True)\n",
    "print(len(indexDrop)) # > 63000\n",
    "del(indexDrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'équivalence sel sodium peux être utilisée pour compléter 124 valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage de la variable pays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nombre de pays par produit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble des pays présent dans le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f\"nombre d'occurrence : {len(data.countries_fr.str.split(',').dropna().explode())}\" )\n",
    "print( f\"cardinalité : {len(data.countries_fr.str.split(',').dropna().explode().value_counts())}\")\n",
    "print(f\"{(n:=10)} pays les plus fréquents\")\n",
    "data.countries_fr.str.split(',').dropna().explode().value_counts().head(n).plot.bar()\n",
    "plt.show()\n",
    "del(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.copy()\n",
    "\n",
    "countriesClassificationFr = {\n",
    "    'France':['Guadeloupe','Guyane','La Réunion','Martinique','Mayotte','Nouvelle-Calédonie','Polynésie française','Saint-Martin','Saint-Pierre-et-Miquelon','Wallis-et-Futuna'],\n",
    "    'Royaume-Uni':['Angleterre','Écosse','pays de Galles','Irlande du Nord'],\n",
    "    'Nouvelle-Zélande':['Îles Cook'],\n",
    "}\n",
    "\n",
    "def countryInclude(df, parent, children):\n",
    "    df.loc[df.countries_fr.str.contains(children) & \n",
    "          ~df.countries_fr.str.contains(parent), 'countries_fr'\n",
    "    ] = df.countries_fr.str.replace(children, f'{parent},{children}')\n",
    "\n",
    "def countryIncludes(df, parent, subset):\n",
    "    for children in subset:\n",
    "        countryInclude(df, parent, children)\n",
    "\n",
    "def countriesIncludes(df, countriesClassification):\n",
    "    for parent, subset in countriesClassification.items():\n",
    "        countryIncludes(df, parent, subset)\n",
    "\n",
    "# countriesIncludes(temp, countriesClassificationFr)\n",
    "# display(temp[data.countries_fr.str.contains('Guadeloupe')].countries_fr.head())\n",
    "del(temp, countriesClassificationFr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.copy()\n",
    "\n",
    "countriesCorrectionFr = {\n",
    "    'Allemagne':['Deutschland','Duitsland',],\n",
    "    'Arabie saoudite':['السعودية',],\n",
    "    'Australie':['Australien',],\n",
    "    'Azerbaïdjan':['Azərbaycan',],\n",
    "    'Bahreïn':['البحرين',],\n",
    "    'Belgique':['Belgie','Belgien',],\n",
    "    'Canada':['Québec'],\n",
    "    'Danemark':['Denemarken',],\n",
    "    'Écosse':['Scotland'],\n",
    "    'États-Unis':['Etats-unis',],\n",
    "    'Espagne':['Spanje','Spanyolorszag',],\n",
    "    'France':['Frankreich','Frankrijk','Puyricard','Franciaorszag',],\n",
    "    'Hong Kong':['香港',],\n",
    "    'Hongrie':['Magyarorszag',],\n",
    "    'Inde':['भारत',],\n",
    "    'Irak':['العراق','Other-العراق',],\n",
    "    'Japon':['日本',],\n",
    "    'Oman':['سلطنة-عمان',],\n",
    "    'Pays-Bas':['Nederland',],\n",
    "    'Pologne':['Szczecin',],\n",
    "    'Portugal':['Portugalia',],\n",
    "    'Québec':['Quebec',],\n",
    "    'République tchèque':['Czech','Tschechien',],\n",
    "    'Royaume-Uni':['Nagy-britannia','المملكة-المتحدة',],\n",
    "    'Suisse':['Zwitserland',],\n",
    "    'Suède':['Zweden',],\n",
    "    'Tunisie':['تونس',],\n",
    "    'Turquie':['Turkiye',],\n",
    "}\n",
    "\n",
    "def countryRemoveDuplicateFr(df):\n",
    "    df.loc[:, 'countries_fr'] = df.countries_fr.apply(\n",
    "        lambda countries: \",\".join(dict.fromkeys(countries.split(','))) )\n",
    "\n",
    "def countryCorrectionFr(df, country, mistake):\n",
    "    df.loc[df.countries_fr.str.contains(mistake),\n",
    "        'countries_fr'] = df.countries_fr.str.replace(mistake, country)\n",
    "\n",
    "def countryCorrectionsFr(df, country, mistakes):\n",
    "    for mistake in mistakes:\n",
    "        countryCorrectionFr(df, country, mistake)\n",
    "\n",
    "def countriesRemoveTagsFr(df):\n",
    "    df.loc[df.countries_fr.str.contains(fr'^.{{2}}:|,.{{2}}:', regex=True), \n",
    "        'countries_fr'] = df.countries_fr.str.replace(fr'.{{2}}:', '', regex=True)\n",
    "\n",
    "def countriesCorrectionsFr(df, countriesCorrection):\n",
    "    countriesRemoveTagsFr(df)\n",
    "    for country, mistakes in countriesCorrection.items():\n",
    "        countryCorrectionsFr(df, country, mistakes)\n",
    "    countryRemoveDuplicateFr(df.dropna())\n",
    "\n",
    "# countriesCorrections(temp, countriesCorrectionFr)\n",
    "\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(data={'countries_fr':['France,États-Unis,en:France']})\n",
    "display(temp.countries_fr)\n",
    "countriesRemoveTagsFr(temp)\n",
    "display(temp.countries_fr)\n",
    "countryRemoveDuplicateFr(temp)\n",
    "display(temp.countries_fr)\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.copy()\n",
    "print(len( temp.countries_fr.dropna().str.split(',').explode().value_counts().index.tolist() ))\n",
    "countriesRemoveTagsFr(temp)\n",
    "print(len( temp.countries_fr.dropna().str.split(',').explode().value_counts() ))\n",
    "countriesCorrectionsFr(temp, countriesCorrectionFr)\n",
    "print(len( temp.countries_fr.dropna().str.split(',').explode().value_counts() ))\n",
    "del(temp, countriesCorrectionFr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage de la variable marque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inférence des valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taux de corrélation entre les valeurs nutritives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[:,'energy_100g':'sodium_100g'].corr().style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hors sodium, tout les variables sont au moins légèrement corrélées à une voire plusieurs d'entre elle. détaillons celle entrant dans le calcul du nutriscore\n",
    "- energy : fat(0.7), saturated-fat(0.6), carbohydrates (0.5), protein(0.3) fiber(0.3)\n",
    "- sugar : carbohydrates(0.7), energy(0.3), protein(0.3)\n",
    "- saturated-fat : fat(0.7), energy(0.6)\n",
    "- proteins : energy(0.3), sugar(0.3), fat(0.2), fiber(0.2)\n",
    "- fibres : energy(0.3), carbohydrates(0.3), proteins(0.2)\n",
    "- sodium : aucune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taux de remplissage des valeurs nutritives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100-food.loc[:,'energy_100g':'sodium_100g'].isna().sum().sort_values()/len(data)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hors fibre et graisse saturé, le taux de valeur manquante est relativement faible : aux alentour de 5% ou moins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conclusion et choix des méthodes d'inférences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "energy, proteins, sugars, carbohydrates, fat, saturated-fat : toute ces variables ont un taux de corrélation suffisamment important au regard du faible taux de valeur manquante pour essayer de les combler par iterativeimputer\n",
    "\n",
    "le sodium : étant faiblement corrélé, l'inférer par iterativeimputer introduirait un bruit sans valeur informative. Une statistique descriptive telle que la médiane permettrait de combler sans influencer la série. Au vue du faible taux de valeur manquante, une valeur non aberrante serait aussi de faible influence sur la série\n",
    "\n",
    "la fibre : le taux élevé de valeur manquantes au regard des autres variables incite à cherche s'il n'y pas de raison à cet écart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inférence du sodium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inférence par le sel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'équivalence sel sodium ne peux être utilisée que pour compléter 2 valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( (food.salt_100g.isna() & ~food.sodium_100g.isna()).sum() )\n",
    "food.loc[food.salt_100g.isna() & ~food.sodium_100g.isna(), 'sodium_100g'] = food.salt_100g/2.54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'information apportée par salt_100g est maintenant entièrement redondante et peux être abandonnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.drop(['salt_100g'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inférence métier / positionnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le sodium est une composante négative du nutriscore et du nutrigrade, inférer par une valeur médiane ou moyenne stable pourrait contribuer à relever le score de produit qui aurait choisit de ne pas préciser dans l'intérêt de leur vente, on choisit de regarder la distribution en fonction du grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food[~food.nutrition_grade_fr.isna()].groupby('nutrition_grade_fr').sodium_100g.agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(food[~food.nutrition_grade_fr.isna() & food.nutrition_grade_fr.isin([*'cde'])].sodium_100g, np.append(np.arange(0,1.5,0.3), 100)).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la formule du nutriscore attribut un nombre de point négatif maximum à partir de 900mg de sel, valeur qui reste assez présente dans la distribution. On choisit de l'attribuer au valeur manquante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[food.sodium_100g.isna(), 'sodium_100g'] = 0.901"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inférence des liquides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : STOP !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inférence des valeurs nutritives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inférence par iterativeimputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "\n",
    "# columnsToImput = ['energy_100g', 'fat_100g', 'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g', 'proteins_100g', 'isBeverage']\n",
    "# imputed = pd.DataFrame(index=food.index, columns=columnsToImput, data=\n",
    "#     IterativeImputer(random_state=0, tol=0.001 ,max_iter=100, missing_values=np.nan).fit_transform(\n",
    "#         food[columnsToImput].replace({pd.NA: np.nan})))\n",
    "\n",
    "# display(food[columnsToImput].describe().loc[['min','max']])\n",
    "# display(imputed.describe().loc[['min','max']])\n",
    "\n",
    "# temp1 = imputed.energy_100g.dropna().apply(\n",
    "#     lambda x: (x<0) or (x>3800)).value_counts()\n",
    "# temp2 = imputed.loc[:,'fat_100g':'proteins_100g'].applymap(\n",
    "#     lambda x: (x<0) or (x>100), na_action='ignore').apply(pd.Series.value_counts)\n",
    "# print(f\"{temp1[True] + temp2.loc[True,:].sum()} valeurs non conformes pour {food[columnsToImput].isna().sum().sum()} valeurs imputées\")\n",
    "# del(temp1, temp2, columnsToImput, imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawToImput = food.isna().any(axis=1)\n",
    "# columnsToImput = ['energy_100g', 'fat_100g', 'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g', 'proteins_100g', 'isBeverage']\n",
    "# imputed = pd.DataFrame(index=food[rawToImput].index, columns=columnsToImput, data=\n",
    "#     IterativeImputer(random_state=0, tol=0.001 ,max_iter=100, missing_values=np.nan\n",
    "#         ).fit(food.loc[~rawToImput, columnsToImput].replace({pd.NA: np.nan})\n",
    "#         ).transform(food.loc[rawToImput,columnsToImput].replace({pd.NA: np.nan})) )\n",
    "        \n",
    "# display(food[columnsToImput].describe().loc[['min','max']])\n",
    "# display(imputed.describe().loc[['min','max']])\n",
    "\n",
    "# temp1 = imputed.energy_100g.dropna().apply(\n",
    "#     lambda x: (x<0) or (x>3800)).value_counts()\n",
    "# temp2 = imputed.loc[:,'fat_100g':'proteins_100g'].applymap(\n",
    "#     lambda x: (x<0) or (x>100), na_action='ignore').apply(pd.Series.value_counts)\n",
    "# print(f\"{temp1[True] if True in temp1.index else 0 + temp2.loc[True,:].sum()} valeurs non conformes pour {food[columnsToImput].isna().sum().sum()} valeurs imputées\")\n",
    "# del(temp1, temp2, rawToImput, columnsToImput, imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with fiber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawToImput = food.isna().any(axis=1)\n",
    "# columnsToImput = ['energy_100g', 'fat_100g', 'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'isBeverage']\n",
    "# imputed = pd.DataFrame(index=food[rawToImput].index, columns=columnsToImput, data=\n",
    "#     IterativeImputer(random_state=0, tol=0.001 ,max_iter=100, missing_values=np.nan\n",
    "#         ).fit(food.loc[~rawToImput, columnsToImput].replace({pd.NA: np.nan})\n",
    "#         ).transform(food.loc[rawToImput,columnsToImput].replace({pd.NA: np.nan})) )\n",
    "        \n",
    "\n",
    "# display(food[columnsToImput].describe().loc[['min','max']])\n",
    "# display(imputed.describe().loc[['min','max']])\n",
    "\n",
    "# temp1 = imputed.energy_100g.dropna().apply(\n",
    "#     lambda x: (x<0) or (x>3800)).value_counts()\n",
    "# temp2 = imputed.loc[:,'fat_100g':'proteins_100g'].applymap(\n",
    "#     lambda x: (x<0) or (x>100), na_action='ignore').apply(pd.Series.value_counts)\n",
    "# print(f\"{temp1[True] if True in temp1.index else 0 + temp2.loc[True,:].sum()} valeurs non conformes pour {food[columnsToImput].isna().sum().sum()} valeurs imputées\")\n",
    "# del(temp1, temp2, rawToImput, columnsToImput, imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "rawToImput = food.isna().any(axis=1)\n",
    "columnsToImput = ['energy_100g', 'fat_100g', 'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'isBeverage']\n",
    "imputed = pd.DataFrame(index=food[rawToImput].index, columns=columnsToImput, data=\n",
    "    IterativeImputer(random_state=0, estimator=KNeighborsRegressor(n_neighbors=3), tol=0.001 ,max_iter=100, missing_values=np.nan\n",
    "        ).fit(food.loc[~rawToImput, columnsToImput].replace({pd.NA: np.nan})\n",
    "        ).transform(food.loc[rawToImput,columnsToImput].replace({pd.NA: np.nan})) )\n",
    "        \n",
    "\n",
    "display(food[columnsToImput].describe().loc[['min','max']])\n",
    "display(imputed.describe().loc[['min','max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### résultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[rawToImput,columnsToImput] = imputed\n",
    "food['isBeverage'] = food['isBeverage'].apply(round).astype(float)\n",
    "display(food[columnsToImput].describe().loc[['min','max']])\n",
    "print(food[columnsToImput].isna().sum().sum())\n",
    "del(rawToImput, columnsToImput, imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage du NutriScore et NutriGrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nourriture Solide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toNutriGradeSolid(NutriScore):\n",
    "    return (\n",
    "        'a' if       NutriScore <= -1 else\n",
    "        'b' if 0  <= NutriScore <=  2 else\n",
    "        'c' if 3  <= NutriScore <=  10 else\n",
    "        'd' if 11 <= NutriScore <=  18 else\n",
    "        'e'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (food.isBeverage == 0.0) & food.nutrition_grade_fr.notna() & food['nutrition-score-fr_100g'].notna()\n",
    "error = (food.loc[mask, 'nutrition-score-fr_100g'].apply(toNutriGradeSolid) != food.loc[mask, 'nutrition_grade_fr'])\n",
    "index = error[error].index\n",
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[index, ['nutrition-score-fr_100g', 'nutrition_grade_fr']] = (pd.NA, pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(mask, error, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nourriture Liquide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inférence des scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inférence du nutriscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawToImputS = food['nutrition-score-fr_100g'].isna()\n",
    "# rawToFitS = ~food['nutrition-score-fr_100g'].isna()\n",
    "# columnsToFitS = ['energy_100g', 'fat_100g', 'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g']\n",
    "# columnToImputS = 'nutrition-score-fr_100g'\n",
    "\n",
    "# models = {\n",
    "#     'unscale' : food[columnsToFitS].copy(),\n",
    "#     'standard' : pd.DataFrame(StandardScaler().fit_transform(food[columnsToFitS]), columns=columnsToFitS, index=food.index),\n",
    "#     'robust' : pd.DataFrame(RobustScaler().fit_transform(food[columnsToFitS]), columns=columnsToFitS, index=food.index),\n",
    "#     'MinMax' : pd.DataFrame(MinMaxScaler().fit_transform(food[columnsToFitS]), columns=columnsToFitS, index=food.index),\n",
    "# }\n",
    "\n",
    "# splits = {model : dict(zip(\n",
    "#     ['xtrain', 'xtest', 'ytrain', 'ytest'], \n",
    "#     train_test_split(models[model].loc[rawToFitS], food.loc[rawToFitS, columnToImputS], random_state=0, train_size=0.8)\n",
    "# )) for model in ['unscale', 'standard', 'robust', 'MinMax']}\n",
    "\n",
    "# nMax = 10\n",
    "# pd.DataFrame({\n",
    "#     model : pd.Series(index=range(1,nMax+1), data=\n",
    "#         [KNeighborsRegressor(n_neighbors=i)\n",
    "#             .fit(splits[model]['xtrain'].astype(float), splits[model]['ytrain'].astype(int))\n",
    "#             .score(splits[model]['xtest'].astype(float), splits[model]['ytest'].astype(int))\n",
    "#         for i in range(1,nMax+1)]) \n",
    "#     for model in ['unscale', 'standard', 'robust', 'MinMax']}).plot()\n",
    "# del(rawToImputS, rawToFitS, columnsToFitS, columnToImputS, models, splits, nMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawToImputS = food['nutrition-score-fr_100g'].isna()\n",
    "rawToFitS = ~food['nutrition-score-fr_100g'].isna()\n",
    "columnsToFitS = ['energy_100g', 'fat_100g', 'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g', 'isBeverage']\n",
    "columnToImputS = 'nutrition-score-fr_100g'\n",
    "\n",
    "models = {\n",
    "    'unscale' : food[columnsToFitS].copy(),\n",
    "    'standard' : pd.DataFrame(StandardScaler().fit_transform(food[columnsToFitS]), columns=columnsToFitS, index=food.index),\n",
    "    'robust' : pd.DataFrame(RobustScaler().fit_transform(food[columnsToFitS]), columns=columnsToFitS, index=food.index),\n",
    "    'MinMax' : pd.DataFrame(MinMaxScaler().fit_transform(food[columnsToFitS]), columns=columnsToFitS, index=food.index),\n",
    "}\n",
    "\n",
    "del(columnsToFitS)\n",
    "splits = {model : dict(zip(\n",
    "    ['xtrain', 'xtest', 'ytrain', 'ytest'], \n",
    "    train_test_split(models[model].loc[rawToFitS], food.loc[rawToFitS, columnToImputS], random_state=0, train_size=0.8)\n",
    ")) for model in ['unscale', 'standard', 'robust', 'MinMax']}\n",
    "nMax = 10\n",
    "pd.DataFrame({\n",
    "    model : pd.Series(index=range(1,nMax+1), data=\n",
    "        [KNeighborsRegressor(n_neighbors=i)\n",
    "            .fit(splits[model]['xtrain'].astype(float), splits[model]['ytrain'].astype(int))\n",
    "            .score(splits[model]['xtest'].astype(float), splits[model]['ytest'].astype(int))\n",
    "        for i in range(1,nMax+1)]) \n",
    "    for model in ['unscale', 'standard', 'robust', 'MinMax']}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMax = 15\n",
    "pd.DataFrame({\n",
    "    'robust' : pd.Series(index=range(1,nMax+1), data=\n",
    "        [KNeighborsRegressor(n_neighbors=i)\n",
    "            .fit(splits['robust']['xtrain'].astype(float), splits['robust']['ytrain'].astype(int))\n",
    "            .score(splits['robust']['xtest'].astype(float), splits['robust']['ytest'].astype(int))\n",
    "        for i in range(1,nMax+1)]) \n",
    "    }).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMax = 100\n",
    "scores = pd.Series([\n",
    "    (lambda xtrain, xtest, ytrain, ytest:\n",
    "        KNeighborsRegressor(n_neighbors=5).fit(xtrain, ytrain).score(xtest, ytest))(\n",
    "            *train_test_split(models['robust'].loc[rawToFitS].astype(float), food.loc[rawToFitS, columnToImputS].astype(int), random_state=rand, train_size=0.8)\n",
    ")for rand in range(nMax)])\n",
    "\n",
    "scores.plot.kde()\n",
    "display(scores.describe()[['mean','std']])\n",
    "del(nMax, scores, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed = pd.Series(index=food[rawToImputS].index, dtype=float, data=\n",
    "    KNeighborsRegressor(n_neighbors=5)\n",
    "        .fit(X=models['robust'].loc[rawToFitS], y=food.loc[rawToFitS, columnToImputS])\n",
    "        .predict(X=models['robust'].loc[rawToImputS]) )\n",
    "imputed.describe().loc[['min', 'max']]\n",
    "del(rawToFitS, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[rawToImputS, columnToImputS] = imputed.apply(round)\n",
    "display(food['nutrition-score-fr_100g'].isna().sum())\n",
    "del(rawToImputS, columnToImputS, imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inférence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inférence du nutrigrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawToImputG = food[food['nutrition_grade_fr'].isna()].index\n",
    "# rawToFitG = food[~food['nutrition_grade_fr'].isna()].index\n",
    "# columnsToFitG = ['nutrition-score-fr_100g', 'isBeverage']\n",
    "# columnToImputG = 'nutrition_grade_fr'\n",
    "\n",
    "# models = {\n",
    "#     'unscale' : food[columnsToFitG].copy(),\n",
    "#     'standard' : pd.DataFrame(StandardScaler().fit_transform(food[columnsToFitG]), columns=columnsToFitG, index=food.index),\n",
    "#     'robust' : pd.DataFrame(RobustScaler().fit_transform(food[columnsToFitG]), columns=columnsToFitG, index=food.index),\n",
    "#     'MinMax' : pd.DataFrame(MinMaxScaler().fit_transform(food[columnsToFitG]), columns=columnsToFitG, index=food.index),\n",
    "# }\n",
    "\n",
    "# splits = {model : dict(zip(\n",
    "#     ['xtrain', 'xtest', 'ytrain', 'ytest'], \n",
    "#     train_test_split(models[model].loc[rawToFitG], food.loc[rawToFitG, columnToImputG], random_state=0, train_size=0.8)\n",
    "# )) for model in ['unscale', 'standard', 'robust', 'MinMax']}\n",
    "\n",
    "# nMax = 10\n",
    "# pd.DataFrame({\n",
    "#     model : pd.Series(index=range(1,nMax+1), data=\n",
    "#         [KNeighborsClassifier(n_neighbors=i)\n",
    "#             .fit(splits[model]['xtrain'].astype(float), splits[model]['ytrain'])\n",
    "#             .score(splits[model]['xtest'].astype(float), splits[model]['ytest'])\n",
    "#         for i in range(1,nMax+1)]) \n",
    "#     for model in ['unscale', 'standard', 'robust', 'MinMax']}).plot()\n",
    "# del(rawToImputG, rawToFitG, columnsToFitG, columnToImputG, models, splits, nMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawToImputG = food[(food.isBeverage==0) & (food['nutrition_grade_fr'].isna())].index\n",
    "# rawToFitG = food[(food.isBeverage==0) & ~food['nutrition_grade_fr'].isna()].index\n",
    "# columnsToFitG = ['nutrition-score-fr_100g']\n",
    "# columnToImputG = 'nutrition_grade_fr'\n",
    "\n",
    "# models = {\n",
    "#     'unscale' : food[columnsToFitG].copy(),\n",
    "#     'standard' : pd.DataFrame(StandardScaler().fit_transform(food[columnsToFitG]), columns=columnsToFitG, index=food.index),\n",
    "#     'robust' : pd.DataFrame(RobustScaler().fit_transform(food[columnsToFitG]), columns=columnsToFitG, index=food.index),\n",
    "#     'MinMax' : pd.DataFrame(MinMaxScaler().fit_transform(food[columnsToFitG]), columns=columnsToFitG, index=food.index),\n",
    "# }\n",
    "\n",
    "\n",
    "# splits = {model : dict(zip(\n",
    "#     ['xtrain', 'xtest', 'ytrain', 'ytest'], \n",
    "#     train_test_split(models[model].loc[rawToFitG], food.loc[rawToFitG, columnToImputG], random_state=0, train_size=0.8)\n",
    "# )) for model in ['unscale', 'standard', 'robust', 'MinMax']}\n",
    "\n",
    "# nMax = 10\n",
    "# pd.DataFrame({\n",
    "#     model : pd.Series(index=range(1,nMax+1), data=\n",
    "#         [KNeighborsClassifier(n_neighbors=i)\n",
    "#             .fit(splits[model]['xtrain'].astype(float), splits[model]['ytrain'])\n",
    "#             .score(splits[model]['xtest'].astype(float), splits[model]['ytest'])\n",
    "#         for i in range(1,nMax+1)]) \n",
    "#     for model in ['unscale', 'standard', 'robust', 'MinMax']}).plot()\n",
    "# del(rawToImputG, rawToFitG, columnsToFitG, columnToImputG, models, splits, nMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawToImputG = food[(food.isBeverage==1) & (food['nutrition_grade_fr'].isna())].index\n",
    "rawToFitG = food[(food.isBeverage==1) & (~food['nutrition_grade_fr'].isna())].index\n",
    "columnCandidates = ['energy_100g', 'fat_100g', 'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g', 'isBeverage']\n",
    "columnsToFitG = ['nutrition-score-fr_100g'] + columnCandidates\n",
    "columnToImputG = 'nutrition_grade_fr'\n",
    "\n",
    "models = {\n",
    "    'unscale' : food[columnsToFitG].copy(),\n",
    "    'standard' : pd.DataFrame(StandardScaler().fit_transform(food[columnsToFitG]), columns=columnsToFitG, index=food.index),\n",
    "    'robust' : pd.DataFrame(RobustScaler().fit_transform(food[columnsToFitG]), columns=columnsToFitG, index=food.index),\n",
    "    'MinMax' : pd.DataFrame(MinMaxScaler().fit_transform(food[columnsToFitG]), columns=columnsToFitG, index=food.index),\n",
    "}\n",
    "\n",
    "splits = {model : dict(zip(\n",
    "    ['xtrain', 'xtest', 'ytrain', 'ytest'], \n",
    "    train_test_split(models[model].loc[rawToFitG], food.loc[rawToFitG, columnToImputG], random_state=3, train_size=0.95)\n",
    ")) for model in ['unscale', 'standard', 'robust', 'MinMax']}\n",
    "nMax = 10\n",
    "pd.DataFrame({\n",
    "    model : pd.Series(index=range(1,nMax+1), data=\n",
    "        [KNeighborsClassifier(n_neighbors=i)\n",
    "            .fit(splits[model]['xtrain'].astype(float), splits[model]['ytrain'])\n",
    "            .score(splits[model]['xtest'].astype(float), splits[model]['ytest'])\n",
    "        for i in range(1,nMax+1)]) \n",
    "    for model in ['unscale', 'standard', 'robust', 'MinMax']}).plot()\n",
    "#del(rawToImputG, rawToFitG, columnsToFitG, columnToImputG, models, splits, nMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawToImputG = food[food['nutrition_grade_fr'].isna()].index\n",
    "# rawToFitG = food[~food['nutrition_grade_fr'].isna()].index\n",
    "# columnCandidates = ['energy_100g', 'fat_100g', 'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g', 'isBeverage']\n",
    "# columnReferences = ['nutrition-score-fr_100g']\n",
    "# columnToImputG = 'nutrition_grade_fr'\n",
    "\n",
    "# robust = pd.DataFrame(RobustScaler().fit_transform(food[columnReferences+columnCandidates]), columns=columnReferences+columnCandidates, index=food.index)\n",
    "\n",
    "# rawtrain, rawtest, ytrain, ytest = train_test_split(rawToFitG, food.loc[rawToFitG, columnToImputG], train_size=0.8, random_state=0)\n",
    "\n",
    "# def KNeighborsClassifierScorer(n, columnsToFit):\n",
    "#     return (KNeighborsClassifier(n_neighbors=n)\n",
    "#         .fit(robust.loc[rawtrain, columnsToFit], ytrain)\n",
    "#         .score(robust.loc[rawtest, columnsToFit], ytest))\n",
    "\n",
    "# nMin, nMax= 4, 15\n",
    "# Results = pd.DataFrame({\n",
    "#             candidate : pd.Series(index=range(nMin, nMax+1), data=[\n",
    "#                 KNeighborsClassifierScorer(n, columnReferences+[candidate])\n",
    "#                 for n in range(nMin,nMax+1)])\n",
    "#             for candidate in columnCandidates})\n",
    "# Results['reference'] = pd.Series(index=range(nMin, nMax+1), data=[KNeighborsClassifierScorer(n, columnReferences) for n in range(nMin,nMax+1)])\n",
    "\n",
    "# fig = plt.figure(dpi=200)\n",
    "# Results.plot(ax = plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawToImputG = food[food['nutrition_grade_fr'].isna()].index\n",
    "# rawToFitG = food[~food['nutrition_grade_fr'].isna()].index\n",
    "# columnCandidates = ['fat_100g', 'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g', 'isBeverage']\n",
    "# columnReferences = ['nutrition-score-fr_100g', 'energy_100g']\n",
    "# columnToImputG = 'nutrition_grade_fr'\n",
    "\n",
    "# robust = pd.DataFrame(RobustScaler().fit_transform(food[columnReferences+columnCandidates]), columns=columnReferences+columnCandidates, index=food.index)\n",
    "\n",
    "# rawtrain, rawtest, ytrain, ytest = train_test_split(rawToFitG, food.loc[rawToFitG, columnToImputG], train_size=0.8, random_state=0)\n",
    "\n",
    "# def KNeighborsClassifierScorer(n, columnsToFit):\n",
    "#     return (KNeighborsClassifier(n_neighbors=n)\n",
    "#         .fit(robust.loc[rawtrain, columnsToFit], ytrain)\n",
    "#         .score(robust.loc[rawtest, columnsToFit], ytest))\n",
    "\n",
    "# nMin, nMax= 4, 15\n",
    "# Results = pd.DataFrame({\n",
    "#             candidate : pd.Series(index=range(nMin, nMax+1), data=[\n",
    "#                 KNeighborsClassifierScorer(n, columnReferences+[candidate])\n",
    "#                 for n in range(nMin,nMax+1)])\n",
    "#             for candidate in columnCandidates})\n",
    "# Results['reference'] = pd.Series(index=range(nMin, nMax+1), data=[KNeighborsClassifierScorer(n, columnReferences) for n in range(nMin,nMax+1)])\n",
    "\n",
    "# fig = plt.figure(dpi=200)\n",
    "# Results.plot(ax = plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawToImputG = food[food['nutrition_grade_fr'].isna()].index\n",
    "# rawToFitG = food[~food['nutrition_grade_fr'].isna()].index\n",
    "# columnCandidates = ['fat_100g', 'saturated-fat_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g', 'isBeverage']\n",
    "# columnReferences = ['nutrition-score-fr_100g', 'energy_100g', 'carbohydrates_100g']\n",
    "# columnToImputG = 'nutrition_grade_fr'\n",
    "\n",
    "# robust = pd.DataFrame(RobustScaler().fit_transform(food[columnReferences+columnCandidates]), columns=columnReferences+columnCandidates, index=food.index)\n",
    "\n",
    "# rawtrain, rawtest, ytrain, ytest = train_test_split(rawToFitG, food.loc[rawToFitG, columnToImputG], train_size=0.8, random_state=0)\n",
    "\n",
    "# def KNeighborsClassifierScorer(n, columnsToFit):\n",
    "#     return (KNeighborsClassifier(n_neighbors=n)\n",
    "#         .fit(robust.loc[rawtrain, columnsToFit], ytrain)\n",
    "#         .score(robust.loc[rawtest, columnsToFit], ytest))\n",
    "\n",
    "# nMin, nMax= 4, 15\n",
    "# Results = pd.DataFrame({\n",
    "#             candidate : pd.Series(index=range(nMin, nMax+1), data=[\n",
    "#                 KNeighborsClassifierScorer(n, columnReferences+[candidate])\n",
    "#                 for n in range(nMin,nMax+1)])\n",
    "#             for candidate in columnCandidates})\n",
    "# Results['reference'] = pd.Series(index=range(nMin, nMax+1), data=[KNeighborsClassifierScorer(n, columnReferences) for n in range(nMin,nMax+1)])\n",
    "\n",
    "# fig = plt.figure(dpi=200)\n",
    "# Results.plot(ax = plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawToImputG = food[food['nutrition_grade_fr'].isna()].index\n",
    "# rawToFitG = food[~food['nutrition_grade_fr'].isna()].index\n",
    "# columnCandidates = ['fat_100g', 'saturated-fat_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g']\n",
    "# columnReferences = ['nutrition-score-fr_100g', 'energy_100g', 'carbohydrates_100g', 'isBeverage']\n",
    "# columnToImputG = 'nutrition_grade_fr'\n",
    "\n",
    "# robust = pd.DataFrame(RobustScaler().fit_transform(food[columnReferences+columnCandidates]), columns=columnReferences+columnCandidates, index=food.index)\n",
    "\n",
    "# rawtrain, rawtest, ytrain, ytest = train_test_split(rawToFitG, food.loc[rawToFitG, columnToImputG], train_size=0.8, random_state=0)\n",
    "\n",
    "# def KNeighborsClassifierScorer(n, columnsToFit):\n",
    "#     return (KNeighborsClassifier(n_neighbors=n)\n",
    "#         .fit(robust.loc[rawtrain, columnsToFit], ytrain)\n",
    "#         .score(robust.loc[rawtest, columnsToFit], ytest))\n",
    "\n",
    "# nMin, nMax= 4, 15\n",
    "# Results = pd.DataFrame({\n",
    "#             candidate : pd.Series(index=range(nMin, nMax+1), data=[\n",
    "#                 KNeighborsClassifierScorer(n, columnReferences+[candidate])\n",
    "#                 for n in range(nMin,nMax+1)])\n",
    "#             for candidate in columnCandidates})\n",
    "# Results['reference'] = pd.Series(index=range(nMin, nMax+1), data=[KNeighborsClassifierScorer(n, columnReferences) for n in range(nMin,nMax+1)])\n",
    "\n",
    "# fig = plt.figure(dpi=200)\n",
    "# Results.plot(ax = plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawToImputG = food[food['nutrition_grade_fr'].isna()].index\n",
    "rawToFitG = food[~food['nutrition_grade_fr'].isna()].index\n",
    "columnCandidates = ['fat_100g', 'saturated-fat_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g']\n",
    "columnReferences = ['nutrition-score-fr_100g', 'energy_100g', 'carbohydrates_100g', 'isBeverage']\n",
    "columnToImputG = 'nutrition_grade_fr'\n",
    "\n",
    "rawtrain, rawtest, ytrain, ytest = train_test_split(rawToFitG, food.loc[rawToFitG, columnToImputG], train_size=0.8, random_state=0)\n",
    "\n",
    "robust = pd.DataFrame(RobustScaler()\n",
    "    .fit(food.loc[rawtrain, columnReferences+columnCandidates])\n",
    "    .transform(food.loc[:, columnReferences+columnCandidates])\n",
    ", columns=columnReferences+columnCandidates, index=food.index)\n",
    "\n",
    "def KNeighborsClassifierScorer(n, columnsToFit):\n",
    "    return (KNeighborsClassifier(n_neighbors=n)\n",
    "        .fit(robust.loc[rawtrain, columnsToFit], ytrain)\n",
    "        .score(robust.loc[rawtest, columnsToFit], ytest))\n",
    "\n",
    "nMin, nMax= 4, 15\n",
    "Results = pd.DataFrame()\n",
    "Results['nutri'] = pd.Series(index=range(nMin, nMax+1), data=[KNeighborsClassifierScorer(n, ['nutrition-score-fr_100g']) for n in range(nMin,nMax+1)])\n",
    "Results['nutri+enr'] = pd.Series(index=range(nMin, nMax+1), data=[KNeighborsClassifierScorer(n, ['nutrition-score-fr_100g', 'energy_100g']) for n in range(nMin,nMax+1)])\n",
    "Results['nutri+enr+car'] = pd.Series(index=range(nMin, nMax+1), data=[KNeighborsClassifierScorer(n, ['nutrition-score-fr_100g', 'energy_100g', 'carbohydrates_100g']) for n in range(nMin,nMax+1)])\n",
    "Results['nutri+enr+car+bev'] = pd.Series(index=range(nMin, nMax+1), data=[KNeighborsClassifierScorer(n, ['nutrition-score-fr_100g', 'energy_100g', 'carbohydrates_100g', 'isBeverage']) for n in range(nMin,nMax+1)])\n",
    "\n",
    "fig = plt.figure(dpi=200)\n",
    "Results.plot(ax = plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test de l'hypothése de perte de ligne dans les ensembles\n",
    "print(\"l'ensemble des lignes dont il faut apprendre plus celle à imputer est constitutif du dataset filtré : \",\n",
    "    (len(rawToImputG)+len(rawToFitG) == len(food)) &\n",
    "     food.index.equals(rawToImputG.union(rawToFitG)) )\n",
    "print(\"l'ensemble des lignes dont il faut apprendre plus celle a tester est constitutif de l'ensemble des nutrigrade non nul : \",\n",
    "    (len(rawtrain)+len(rawtest) == food['nutrition_grade_fr'].notna().sum()) &\n",
    "     rawToFitG.equals(rawtrain.union(rawtest)) )\n",
    "print(\"l'ensemble des ligne mis à l'échelle est constitutif du dataset filtré : \",\n",
    "    (len(robust) == len(food)) &\n",
    "     robust.index.equals(food.index) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test de l'hypothése d'un problème d'index\n",
    "rawToImputG = food[food['nutrition_grade_fr'].isna()].index\n",
    "rawToFitG = food[~food['nutrition_grade_fr'].isna()].index\n",
    "columnCandidates = ['fat_100g', 'saturated-fat_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g']\n",
    "columnReferences = ['nutrition-score-fr_100g', 'energy_100g', 'carbohydrates_100g', 'isBeverage']\n",
    "columnToImputG = 'nutrition_grade_fr'\n",
    "\n",
    "rawtrain, rawtest, ytrain, ytest = train_test_split(rawToFitG, food.loc[rawToFitG, columnToImputG], train_size=0.8, random_state=0)\n",
    "\n",
    "robust = pd.DataFrame((scaler := RobustScaler())\n",
    "    .fit(food.loc[rawtrain, columnReferences+columnCandidates])\n",
    "    .transform(food.loc[:, columnReferences+columnCandidates])\n",
    ", columns=columnReferences+columnCandidates, index=food.index)\n",
    "\n",
    "reverse = pd.DataFrame(scaler.inverse_transform(robust)\n",
    ", columns=columnReferences+columnCandidates, index=food.index)\n",
    "\n",
    "display(\"test\", data[['nutrition-score-fr_100g', 'energy_100g', 'carbohydrates_100g', 'fat_100g', 'saturated-fat_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g']].loc[12:15])\n",
    "display(food[['nutrition-score-fr_100g', 'energy_100g', 'carbohydrates_100g', 'fat_100g', 'saturated-fat_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g']].loc[12:15])\n",
    "display(reverse[['nutrition-score-fr_100g', 'energy_100g', 'carbohydrates_100g', 'fat_100g', 'saturated-fat_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'sodium_100g']].loc[12:15])\n",
    "\n",
    "del(reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP HERE !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(rawToImputG, rawToFitG, columnToImputG, robust, rawtrain, rawtest, ytrain, ytest, nMax, Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawToImputG = food['nutrition_grade_fr'].isna()\n",
    "rawToFitG = ~food['nutrition_grade_fr'].isna()\n",
    "columnsToFitG = ['nutrition-score-fr_100g']\n",
    "columnToImputG = 'nutrition_grade_fr'\n",
    "\n",
    "imputed = pd.Series(index=food[rawToImputG].index, dtype=pd.StringDtype(), data=\n",
    "    KNeighborsClassifier(n_neighbors=12\n",
    "        ).fit(X=food.loc[rawToFitG, columnsToFitG].replace({pd.NA: np.nan}), y=food.loc[rawToFitG, columnToImputG].replace({pd.NA: np.nan})\n",
    "        ).predict(X=food.loc[rawToImputG, columnsToFitG].replace({pd.NA: np.nan})) )\n",
    "imputed.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.loc[rawToImputG, columnToImputG] = imputed\n",
    "food.nutrition_grade_fr.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse exploratoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univarié et bivarié"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['nutrition_grade_fr', 'nutrition-score-fr_100g', 'energy_100g', 'sugars_100g', 'saturated-fat_100g', 'sodium_100g', 'fiber_100g', 'proteins_100g']\n",
    "sns.pairplot(food.groupby('nutrition_grade_fr').sample(100)[cols], hue='nutrition_grade_fr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivarié"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_pca = food.loc[~food.product_name.isna(),'energy_100g':'nutrition-score-fr_100g']\n",
    "\n",
    "# Centrage et Réduction\n",
    "X = preprocessing.StandardScaler().fit_transform(food_pca)\n",
    "\n",
    "# Calcul des composantes principales\n",
    "pca = decomposition.PCA(n_components=len(food_pca.columns)).fit(X)\n",
    "\n",
    "# Eboulis des valeurs propres\n",
    "display_scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_ext autoreload\n",
    "#autoreload 2\n",
    "from functions import *\n",
    "\n",
    "# Cercle des corrélations\n",
    "pcs = pca.components_\n",
    "display_circles(pca.components_, len(food_pca.columns), pca, [(0,1),(2,3),(4,5)], labels = food_pca.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection des individus\n",
    "food_grade = food[food['nutrition_grade_fr'].isin(list('ace'))]\n",
    "food_pca = food_grade.groupby('nutrition_grade_fr').sample(100).loc[:,'energy_100g':'nutrition-score-fr_100g']\n",
    "\n",
    "# Centrage et Réduction\n",
    "X = preprocessing.StandardScaler().fit_transform(food_pca)\n",
    "\n",
    "# Calcul des composantes principales\n",
    "pca = decomposition.PCA(n_components=len(food_pca.columns)).fit(X)\n",
    "\n",
    "display_factorial_planes(pca.transform(X), len(food_pca.columns), pca, [(0,1),(2,3),(4,5)], alpha=1,\n",
    "    illustrative_var = np.array(food.loc[food_pca.index, 'nutrition_grade_fr'].replace({pd.NA: np.nan})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anova - mange t'on aussi bien dans les 6 pays les plus représentés ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topCountries = data.countries_fr.str.split(',').explode().value_counts().head(6).index.tolist()\n",
    "topCountries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_df = pd.DataFrame({\n",
    "    'country' : [country for country in topCountries for i in range(1000) ],\n",
    "    'score' : pd.concat([food.loc[food.countries_fr.str.contains(country), 'nutrition-score-fr_100g'].sample(1000).astype(float) for country in topCountries]).reset_index(drop=True) })\n",
    "print(anova_df.score.mean())\n",
    "anova_df.groupby('country').mean().sort_values('score').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on se pose ici la question de savoir si le nutriscore Connaît une variation significative selon les pays. Pour les pays étudiés la moyenne globale est de **8.3**, allant de **6.39** pour l'espagne à **9.23** pour l'allemagne, le score le plus bas étant un meilleur score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='country', y='score', data=anova_df, showmeans=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se pose la question de la significativité de ces écarts avec une possibilité qu'ils soient du au hasard. au vue la taille des échantillons, on se fixe un seuil de 0.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0 : on mange aussi bien en moyenne dans chaqu'un des 5 pays les plus représentés  \n",
    "<=> U(us) = U(fr) = U(ch) = U(de) = U(en) = U(es)\n",
    "\n",
    "H1 : on ne mange pas aussi bien en moyenne dans chaqu'un des 5 pays les plus représentés  \n",
    "<=> Toute les moyennes ne sont pas égales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "model = ols('score ~ C(country)', data=anova_df).fit()\n",
    "aov_table = sm.stats.anova_lm(model, typ=2)\n",
    "aov_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on admet donc qu'il existe une différence statistiquement significative entre les moyennes du nutriscore selon les pays. On se pose la question de savoir si cette différence apporte une information pertinente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_table(aov):\n",
    "    aov['mean_sq'] = aov[:]['sum_sq']/aov[:]['df']\n",
    "    aov['eta_sq'] = aov[:-1]['sum_sq']/sum(aov['sum_sq'])\n",
    "    aov['omega_sq'] = (aov[:-1]['sum_sq']-(aov[:-1]['df']*aov['mean_sq'][-1]))/(sum(aov['sum_sq'])+aov['mean_sq'][-1])\n",
    "    return aov[['sum_sq', 'df', 'mean_sq', 'F', 'PR(>F)', 'eta_sq', 'omega_sq']]\n",
    "\n",
    "anova_table(aov_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avec un eta carré inférieur à 0.01, on conclue que si les différences sont significative, elle ne sont pas porteuse de sens et on rejette la corrélation entre nutriscore et pays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Autre ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreByCountry(country=None):\n",
    "    score = data[['nutrition_grade_fr', 'nutrition-score-fr_100g']]\n",
    "    if country:\n",
    "        score = data[data.countries.str.contains(country)][['nutrition_grade_fr', 'nutrition-score-fr_100g']]\n",
    "    else :\n",
    "        score = data[['nutrition_grade_fr', 'nutrition-score-fr_100g']]\n",
    "    score = score.assign( grade_or_score=score['nutrition_grade_fr'].combine_first(score['nutrition-score-fr_100g']))\n",
    "    pd.concat(axis=1, objs=[\n",
    "        (100-score.isnull().sum()/len(score) * 100)[::-1],\n",
    "        (score.isnull().sum()/len(score) * 100)[::-1]]\n",
    "    ).rename(columns={0:'not null',1:'null'}\n",
    "    ).plot(kind='barh', stacked=True, color=['#306645','#4c4c4c'], figsize=(10,2) #green, gray\n",
    "    ).legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "scoreByCountry()\n",
    "scoreByCountry(country='France')\n",
    "scoreByCountry(country='États-Unis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data[data['nutrition-score-fr_100g'].isnull()][[\n",
    "    'energy_100g', 'sugars_100g', 'saturated-fat_100g', 'sodium_100g', 'fruits-vegetables-nuts_100g', 'fiber_100g', 'proteins_100g'\n",
    "]].copy()\n",
    "\n",
    "mostFilled = (1-temp.isnull().sum().sort_values() / len(temp)).index.tolist()\n",
    "temp = temp[mostFilled]\n",
    "\n",
    "sns.heatmap(temp.isnull().sort_values(mostFilled, ascending=False).reset_index().drop('index', axis=1))\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data[data['nutrition-score-fr_100g'].isnull()][[\n",
    "    'energy_100g', 'sugars_100g', 'saturated-fat_100g', 'sodium_100g', 'fruits-vegetables-nuts_100g', 'fiber_100g', 'proteins_100g'\n",
    "]].copy()\n",
    "\n",
    "temp = data.loc[temp[temp.isnull().all(axis=1)].index]\n",
    "print(len(temp))\n",
    "display(pd.cut(100-temp.isna().sum().sort_values()/len(temp)*100, range(10, 101, 10)).dropna())\n",
    "\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data[data['nutrition-score-fr_100g'].isnull()][[\n",
    "    'energy_100g', 'sugars_100g', 'saturated-fat_100g', 'sodium_100g', 'fruits-vegetables-nuts_100g', 'fiber_100g', 'proteins_100g'\n",
    "]].copy()\n",
    "\n",
    "temp = data.loc[temp[temp.isnull().all(axis=1)].index][[\n",
    "    'pnns_groups_2', 'pnns_groups_1', 'categories_fr', 'main_category_fr', 'ingredients_text', 'labels_fr'\n",
    "]].copy()\n",
    "\n",
    "mostFilled = (1-temp.isnull().sum().sort_values() / len(temp)).index.tolist()\n",
    "sns.heatmap(temp[mostFilled].isnull().sort_values(mostFilled, ascending=False).reset_index().drop('index', axis=1))\n",
    "\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data[data['nutrition-score-fr_100g'].isnull()][[\n",
    "    'energy_100g', 'sugars_100g', 'saturated-fat_100g', 'sodium_100g', 'fruits-vegetables-nuts_100g', 'fiber_100g', 'proteins_100g'\n",
    "]].copy()\n",
    "\n",
    "temp = data.loc[temp[temp.isnull().all(axis=1)].index][[\n",
    "    'pnns_groups_2', 'pnns_groups_1', 'categories_fr', 'main_category_fr', 'ingredients_text', 'labels_fr'\n",
    "]].copy()\n",
    "\n",
    "# print( data.countries_fr.str.split(',').dropna().apply(len).value_counts().sort_index(kind = 'mergesort') )\n",
    "# temp.categories_fr.str.split(',').dropna().value_counts()\n",
    "\n",
    "# 183 : display(len(data[data.categories_fr.str.contains('Non alimentaire')]))\n",
    "print(len(temp))\n",
    "print(temp.pnns_groups_2.dropna().shape)\n",
    "display(temp.pnns_groups_2.value_counts())\n",
    "\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data[data['nutrition-score-fr_100g'].isnull()][[\n",
    "    'sugars_100g', 'carbohydrates_100g'\n",
    "]].copy()\n",
    "\n",
    "display( data[['sugars_100g', 'carbohydrates_100g']].corr().style.background_gradient(cmap='coolwarm') )\n",
    "display( temp[['sugars_100g', 'carbohydrates_100g']].corr().style.background_gradient(cmap='coolwarm') )\n",
    "sns.heatmap(temp.isnull())\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data[data['nutrition-score-fr_100g'].isnull()][[\n",
    "    'saturated-fat_100g', 'fat_100g', 'cholesterol_100g', 'trans-fat_100g'\n",
    "]].copy()\n",
    "\n",
    "display( data[['saturated-fat_100g', 'fat_100g', 'cholesterol_100g', 'trans-fat_100g']].corr().style.background_gradient(cmap='coolwarm') )\n",
    "display( temp[['saturated-fat_100g', 'fat_100g', 'cholesterol_100g', 'trans-fat_100g']].corr().style.background_gradient(cmap='coolwarm') )\n",
    "sns.heatmap(temp.isnull())\n",
    "del(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data[data['nutrition-score-fr_100g'].isnull()][[\n",
    "    'fiber_100g','carbohydrates_100g', 'sugars_100g', 'vitamin-a_100g', 'iron_100g'\n",
    "]].copy()\n",
    "\n",
    "display( data[['fiber_100g','carbohydrates_100g', 'sugars_100g', 'vitamin-a_100g', 'iron_100g']].corr().style.background_gradient(cmap='coolwarm') )\n",
    "display( temp[['fiber_100g','carbohydrates_100g', 'sugars_100g', 'vitamin-a_100g', 'iron_100g']].corr().style.background_gradient(cmap='coolwarm') )\n",
    "sns.heatmap(temp.isnull())\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.isna().sum()/len(data)*100).plot.hist(cumulative=True, bins=range(0, 101, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorie = data[['categories', 'categories_tags', 'categories_fr','pnns_groups_1', 'pnns_groups_2', 'labels', 'labels_tags', 'labels_fr']]\n",
    "pd.concat(axis=1, objs=[\n",
    "    (100-categorie.isnull().sum()/len(categorie) * 100)[::-1],\n",
    "    (categorie.isnull().sum()/len(categorie) * 100)[::-1]]\n",
    ").rename(columns={0:'not null',1:'null'}\n",
    ").plot(kind='barh', stacked=True, color=['#306645','#4c4c4c'], figsize=(10,6) #green, gray\n",
    ").legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscore = data[data.nutrition_grade_fr.isnull()].categories_tags\n",
    "print (f\"defined : {len(unscore)/len(data)*100}, numbers of bin : {len(score.value_counts())}\")\n",
    "display(unscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcul = data[['energy_100g', 'sugars_100g', 'saturated-fat_100g', 'sodium_100g', 'fruits-vegetables-nuts_100g', 'fiber_100g', 'proteins_100g']]\n",
    "pd.concat(axis=1, objs=[\n",
    "    (100-calcul.isnull().sum()/len(calcul) * 100)[::-1],\n",
    "    (calcul.isnull().sum()/len(calcul) * 100)[::-1]]\n",
    ").rename(columns={0:'not null',1:'null'}\n",
    ").plot(kind='barh', stacked=True, color=['#306645','#4c4c4c'], figsize=(10,6) #green, gray\n",
    ").legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,'fat_100g':].dropna(axis=1, how='all').corr().fat_100g.sort_values(ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['capric-acid_100g'].isnull().sum()/len(data) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,'fat_100g':].dropna(axis=1, how='all').corr()['saturated-fat_100g'].sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['chloride_100g'].isnull().sum()/len(data) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les valeurs étant donnés pour 100g, elles doivent être comprise entre 0 et 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la quantité d'énergie max par gramme étant de 37.7kJ, la quantité d'énergie max pour 100g et 3770Kj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in data_pca.loc[:,'energy_100g':'energy-from-fat_100g']:\n",
    "    data_pca.drop( data_pca.loc[ (data_pca[name]<0) | (data_pca[name]>3830)].index, inplace=True)\n",
    "display(data_pca.loc[:,'energy_100g':'energy-from-fat_100g'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les déviations standard !?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in data_pca.loc[:,'fat_100g':]:\n",
    "    # print(name)\n",
    "    # display(data.loc[ data_pca[data_pca[name] >= 99].index])\n",
    "# data[data_pca['calcium_100g'] >= 99]\n",
    "data.serving_size.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_pca.loc[:,:'ingredients_that_may_be_from_palm_oil_n'].describe())\n",
    "display(data_pca.loc[:,'energy_100g':].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = data.select_dtypes(np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trashbin - TODO : delete\n",
    "\n",
    "# suppression des lignes avec code barre dupliqué\n",
    "# data = data[~data.index.duplicated(keep='first')]\n",
    "# data = data[~data.index.isna()]\n",
    "# data[data.index.isna()]\n",
    "\n",
    "# selection des valeurs numériques pour pca\n",
    "# data_pca = data.select_dtypes(include = np.number).copy()\n",
    "\n",
    "# drop des colones ne contenant \n",
    "# data_pca.drop(data_pca.count().where(lambda x: x<=1).dropna().index, axis='columns', inplace=True) #vérifier dropna ?\n",
    "# display (data_pca.count().where(lambda x: x<=1).dropna().index) #vérifier dropna ?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "356a1c1c710f8bc386987db434be67f80e0e163152f229b0d82e7b46307af9d9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
